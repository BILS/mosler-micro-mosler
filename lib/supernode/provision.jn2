# -*-sh-*-

echo "Configuring iptables"
sed -i "/^-A INPUT -m state --state NEW -s {{ env['MGMT_CIDR']|replace('/','\/') }} -j ACCEPT/ d" /etc/sysconfig/iptables
# Insert it before the other line
sed -i "/^-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT/ i \
-A INPUT -m state --state NEW -s {{ env['MGMT_CIDR']|replace('/','\/') }} -j ACCEPT" /etc/sysconfig/iptables
systemctl restart iptables


##############################################################
# Adding ntp1.it.uu.se. No DNS => ip only
sed -i "/server 130.238.15.1 iburst/ d" /etc/chrony.conf
echo "server 130.238.15.1 iburst" >> /etc/chrony.conf

systemctl enable chronyd.service
systemctl restart chronyd.service

##############################################################
# Mount the nfs share as /meles

mkdir -p {{ env['CAW_DATA'] }} {{ env['MM_PROJECTS'] }}

if mount | grep -q {{ env['CAW_DATA'] }} ;then umount {{ env['CAW_DATA'] }}; fi
if mount | grep -q {{ env['MM_PROJECTS'] }} ;then umount {{ env['MM_PROJECTS'] }}; fi
sleep 5

mount -t nfs storage:/mnt/data {{ env['CAW_DATA'] }} || exit 1
mount -t nfs storage:/mnt/projects {{ env['MM_PROJECTS'] }} || exit 1
sed -i -e '/storage:/ d' /etc/fstab
echo "storage:{{ env['NFS_ROOT'] }}/data {{ env['CAW_DATA'] }}  nfs   auto,noatime,nolock,bg,nfsvers=4,intr,tcp,actimeo=1800 0 0" >> /etc/fstab
echo "storage:{{ env['NFS_ROOT'] }}/projects {{ env['MM_PROJECTS'] }}  nfs   auto,noatime,nolock,bg,nfsvers=4,intr,tcp,actimeo=1800 0 0" >> /etc/fstab


##############################################################
# Munge and Slurm
# See: https://wiki.fysik.dtu.dk/niflheim/SLURM
for package in -plugins '' -devel -munge -perlapi -sjobexit -sjstat -torque -pam_slurm # -openlava -seff -slurmdbd -slurmdb-direct -sql 
do
    if ! rpm -q slurm${package} >/dev/null; then
	rpm -ivh /home/centos/{{ env['VAULT'] }}/slurm/slurm${package}-16.05.4-1.el7.centos.x86_64.rpm
    fi
done

# Nuke the logs
mkdir -p /var/log/{munge,slurm}
for log in munge/munged.log slurm/slurmctld.log  slurm/slurm_job{acct,comp}.log; do :> /var/log/$log; done
:> /var/log/munge/munged.log
chown -R munge:munge /var/log/munge
:> /var/log/slurm/slurmd.log
chown -R slurm:slurm /var/log/slurm


systemctl restart munge.service
systemctl restart slurmctld.service
# No need to add to iptables: ports 6817/udp 6817/tcp 6818/tcp 7321/tcp
systemctl enable munge.service
systemctl enable slurmctld.service

##############################################################
export http_proxy="{{ env['UU_PROXY'] }}"
git config --global http.proxy ${http_proxy}
git config --global https.proxy ${http_proxy/http/https}

declare -A PARTITIONS
PARTITIONS=(\
    ['mm']='compute1,compute2,compute3' \
    ['epouta']='epouta1,epouta2,epouta3' \
    ['mm2-epouta1']='compute1,compute2,epouta1' \
    ['epouta2-mm1']='epouta2,epouta3,compute3' \
    ['mm1-epouta2']='compute3,epouta2,epouta3' \
    ['epouta1-mm2']='epouta1,compute1,compute2' \
)

rm -f ~/.nextflow.log
#rm -rf ~/.nextflow # Don't clean this one, that's where the dependencies are

###################################################
# Git project CAW
# See: https://github.com/SciLifeLab/CAW/

# pre-processing. Nasty part that requires a lot of CPU and temporary disk space
# once we have the pre-processed reads, we run the remainder (variant call, finding mutations)
if [ ! -d {{ env['MM_PROJECTS'] }}/CAW ]; then
    git clone https://github.com/SciLifeLab/CAW.git {{ env['MM_PROJECTS'] }}/CAW
fi

rm -f {{ env['MM_PROJECTS'] }}/CAW/.nextflow.history {{ env['MM_PROJECTS'] }}/CAW/.nextflow.log*
rm -rf {{ env['MM_PROJECTS'] }}/CAW/{work,Preprocessing}

perl -i -pe's/cpus 8/cpus 4/g' {{ env['MM_PROJECTS'] }}/CAW/MultiFQtoVC.nf
perl -i -pe's/memory { 16.GB \* task.attempt }/memory { 4.GB \* task.attempt }/g' {{ env['MM_PROJECTS'] }}/CAW/MultiFQtoVC.nf


for partition in ${!PARTITIONS[@]}; do
    cat > {{ env['MM_PROJECTS'] }}/CAW/config/${partition}.config <<EOF
/* -------------------------------------------------
 * Nextflow config file for CAW project
 * Set up with test data
 * -------------------------------------------------
 * Should be saved either with Nextflow installation
 * or as file ~/.nextflow/config
 */

process {
  executor = 'slurm'
  cpus = 1
  memory = '2 GB'
  time = '1h'
  queue = '${partition}'
  clusterOptions = {
    '-t 60:00 --export=NONE --get-user-env -c 4 --mem 4096'
  }
}

/* This option particularly important for jobs that are submitted on
one cluster and execute on a different cluster (e.g. with different
paths). By default all environment variables are propagated. If the
argument is NONE or specific environment variable names, then the
--get-user-env */

// By default we are turning on trace and timeline tracking
trace {
	enabled = true
	fields = 'process,task_id,name,attempt,status,exit,realtime,%cpu,rss,submit,start,complete,duration,realtime,rchar,wchar'
        file = '~/${partition}.trace'
}

timeline {
	enabled = true
        file = '~/${partition}.timeline.html'
}

params {
  sample       = '{{ env['MM_PROJECTS'] }}/CAW/data/tsv/tiny.tsv'
  gender       = 'XY'
  genome       = '{{ env['CAW_DATA'] }}/human_g1k_v37_decoy.fasta'
  genomeIndex  = '{{ env['CAW_DATA'] }}/human_g1k_v37_decoy.fasta.fai'
  mantaRef     = '{{ env['CAW_DATA'] }}/jesper-human_g1k_v37_decoy.fasta'
  mantaIndex   = '{{ env['CAW_DATA'] }}/jesper-human_g1k_v37_decoy.fasta.fai'
  genomeDict   = '{{ env['CAW_DATA'] }}/human_g1k_v37_decoy.dict'
  cosmic41     = '{{ env['CAW_DATA'] }}/b37_cosmic_v74.noCHR.sort.4.1.vcf'
  cosmicIndex41= '{{ env['CAW_DATA'] }}/b37_cosmic_v74.noCHR.sort.4.1.vcf.idx'
  cosmic       = '{{ env['CAW_DATA'] }}/b37_cosmic_v74.vcf'
  cosmicIndex  = '{{ env['CAW_DATA'] }}/b37_cosmic_v74.vcf.idx'
  dbsnp        = '{{ env['CAW_DATA'] }}/dbsnp_138.b37.vcf'
  dbsnpIndex   = '{{ env['CAW_DATA'] }}/dbsnp_138.b37.vcf.idx'
  kgIndels     = '{{ env['CAW_DATA'] }}/1000G_phase1.indels.b37.vcf'
  kgIndex      = '{{ env['CAW_DATA'] }}/1000G_phase1.indels.b37.vcf.idx'
  millsIndels  = '{{ env['CAW_DATA'] }}/Mills_and_1000G_gold_standard.indels.b37.vcf'
  millsIndex   = '{{ env['CAW_DATA'] }}/Mills_and_1000G_gold_standard.indels.b37.vcf.idx'
  mutect1Home  = '/mnt/sw/mutect1'
  snpeffHome   = '/mnt/sw/snpEff'
  snpeffDb     = 'GRCh37.75'
  picardHome   = '/mnt/sw/picard'
  gatkHome     = '/mnt/sw/GATK-3.3.0'
  vardictHome  = '/mnt/sw/VarDictJava'
  mutect2Home  = '/mnt/sw/GATK-3.6'
  strelkaHome  = '/usr/local/strelka-1.0.15'
  strelkaCFG   = '{{ env['MM_PROJECTS'] }}/CAW/vc/strelka/strelka_config.ini'
  SNIC_tmp_dir = '/tmp'
  intervals    = '{{ env['MM_PROJECTS'] }}/CAW/repeats/centromeres.list'
  acLoci       = '{{ env['CAW_DATA'] }}/1000G_phase3_20130502_SNP_maf0.3.loci'
}
EOF
done

# export NXF_WORK={{ env['MM_PROJECTS'] }}/CAW/work

cat > /usr/local/bin/run-CAW.sh <<EOF
cd {{ env['MM_PROJECTS'] }}/CAW
for partition in ${!PARTITIONS[@]}; do
    NXF_DEBUG=1 && nextflow run -with-dag ~/${partition}.dot /mnt/projects/CAW/MultiFQtoVC.nf -c /mnt/projects/CAW/config/${partition}.config --sample /mnt/projects/CAW/data/tsv/sample.tsv > ~/${partition}.log
done
EOF
chmod +x /usr/local/bin/run-CAW.sh


###################################################
# WGS-structvar project
# See: https://github.com/NBISweden/wgs-structvar

if [ ! -d {{ env['MM_PROJECTS'] }}/WGS ]; then
    git clone https://github.com/NBISweden/wgs-structvar {{ env['MM_PROJECTS'] }}/WGS
fi

#rm -rf ~/.nextflow # Don't clean this one, that's where the dependencies are
rm -f {{ env['MM_PROJECTS'] }}/WGS/.nextflow.history {{ env['MM_PROJECTS'] }}/WGS/.nextflow.log*
rm -rf {{ env['MM_PROJECTS'] }}/WGS/work

# export NXF_WORK={{ env['MM_PROJECTS'] }}/WGS/work

# TODO: Update nextflow.config
# --bam BioInfo/data/CEUTrio.HiSeq.WGS.b37.NA12878.bam --steps manta
# slurm partitions too

cat > /usr/local/bin/run-WGS.sh <<EOF
cd {{ env['MM_PROJECTS'] }}/WGS
for partition in ${!PARTITIONS[@]}; do
    NXF_DEBUG=1 && nextflow run -with-dag ~/${partition}.dot main.nf > ~/${partition}.log
done
EOF
chmod +x /usr/local/bin/run-WGS.sh

###################################################
# SOB stress test
# See: https://www.pdc.kth.se/~pek/sob/README

declare -a SOB_TESTS=(\
'sob -w -b 8m -s 24g' \
'sob -w -b 64k -s 64k -n 500000 -o 5000' \
'sob -rw -b 128k -s 16g -n 100' \
'sob -w -R 5000 -n 50 -s 128m -b 64k' \
)

rm -rf {{ env['MM_PROJECTS'] }}/SOB/{work,run}
mkdir -p {{ env['MM_PROJECTS'] }}/SOB/{work,run}

for partition in ${!PARTITIONS[@]}; do
    for i in ${!SOB_TESTS[@]}; do
	cat > {{ env['MM_PROJECTS'] }}/SOB/run/sbatch-${i}-${partition}.sh <<EOF
#!/bin/bash -l

#SBATCH -p ${partition}
#SBATCH -c 4        
#SBATCH -t 00:20:00  
#SBATCH -J sob-${i}-${partition}
#SBATCH -o {{ env['MM_PROJECTS'] }}/SOB/run/sob-${i}-${partition}.log
#SBATCH -L {{ env['MM_PROJECTS'] }}/SOB/work

srun ${SOB_TESTS[$i]}
EOF
    done
done     

cat > /usr/local/bin/run-SOB.sh <<EOF
for f in {{ env['MM_PROJECTS'] }}/SOB/run/*.sh
do
	sbatch $f
done
EOF
chmod +x /usr/local/bin/run-SOB.sh
